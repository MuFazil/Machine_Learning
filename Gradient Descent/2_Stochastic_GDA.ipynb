{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SGD, instead of using the entire dataset for each iteration, only a single random training example (or a small batch) is selected to calculate the gradient and update the model parameters. This random selection introduces randomness into the optimization process, hence the term “stochastic” in stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of using SGD is its computational efficiency, especially when dealing with large datasets. By using a single example or a small batch, the computational cost per iteration is significantly reduced compared to traditional Gradient Descent methods that require processing the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization: Randomly initialize the parameters of the model.\n",
    "\n",
    "Set Parameters: Determine the number of iterations and the learning rate (alpha) for updating the parameters.\n",
    "\n",
    "Stochastic Gradient Descent Loop: Repeat the following steps until the model converges or reaches the maximum number of iterations: \n",
    "\n",
    "    Shuffle the training dataset to introduce randomness. \n",
    "    Iterate over each training example (or a small batch) in the shuffled order. \n",
    "\n",
    "\n",
    "    Compute the gradient of the cost function with respect to the model parameters using the current training   example (or batch).\n",
    "\n",
    "    Update the model parameters by taking a step in the direction of the negative gradient, scaled by the learning rate. \n",
    "\n",
    "    Evaluate the convergence criteria, such as the difference in the cost function between iterations of the gradient.\n",
    "\n",
    "Return Optimized Parameters: Once the convergence criteria are met or the maximum number of iterations is reached, return the optimized model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01, epochs=1000, batch_size=32, tol=1e-3):\n",
    "        self.learning_rate = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.tolerance = tol\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def mean_squared_error(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def gradient(self, X_batch, y_batch):\n",
    "        y_pred = self.predict(X_batch)\n",
    "        error = y_pred - y_batch\n",
    "        gradient_weights = np.dot(X_batch.T, error) / X_batch.shape[0]\n",
    "        gradient_bias = np.mean(error)\n",
    "        return gradient_weights, gradient_bias\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.random.randn(n_features)\n",
    "        self.bias = np.random.randn()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i+self.batch_size]\n",
    "                y_batch = y_shuffled[i:i+self.batch_size]\n",
    "\n",
    "                gradient_weights, gradient_bias = self.gradient(X_batch, y_batch)\n",
    "                self.weights -= self.learning_rate * gradient_weights\n",
    "                self.bias -= self.learning_rate * gradient_bias\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                y_pred = self.predict(X)\n",
    "                loss = self.mean_squared_error(y, y_pred)\n",
    "                print(f\"Epoch {epoch}: Loss {loss}\")\n",
    "\n",
    "            if np.linalg.norm(gradient_weights) < self.tolerance:\n",
    "                print(\"Convergence reached.\")\n",
    "                break\n",
    "\n",
    "        return self.weights, self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 61.884956955571326\n",
      "Epoch 100: Loss 0.1486698019966988\n",
      "Epoch 200: Loss 0.007979306585507424\n",
      "Epoch 300: Loss 0.007531296326646648\n",
      "Epoch 400: Loss 0.007534300140446138\n",
      "Epoch 500: Loss 0.00754137816353128\n",
      "Epoch 600: Loss 0.007529830981743999\n",
      "Epoch 700: Loss 0.0075319113359766125\n",
      "Epoch 800: Loss 0.007527451814418083\n",
      "Epoch 900: Loss 0.007544075680036645\n"
     ]
    }
   ],
   "source": [
    "# Create random dataset with 100 rows and 5 columns\n",
    "X = np.random.randn(100, 5)\n",
    "# create corresponding target value by adding random\n",
    "# noise in the dataset\n",
    "y = np.dot(X, np.array([1, 2, 3, 4, 5]))\\\n",
    "    + np.random.randn(100) * 0.1\n",
    "# Create an instance of the SGD class\n",
    "model = SGD(lr=0.01, epochs=1000,\n",
    "            batch_size=32, tol=1e-3)\n",
    "w,b=model.fit(X,y)\n",
    "# Predict using predict method from model\n",
    "y_pred = w*X+b\n",
    "#y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD) using TensorFLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 59.99106979370117\n",
      "Epoch 100: Loss 54.841373443603516\n",
      "Epoch 200: Loss 54.756561279296875\n",
      "Epoch 300: Loss 54.793907165527344\n",
      "Epoch 400: Loss 54.72361373901367\n",
      "Epoch 500: Loss 54.76445007324219\n",
      "Epoch 600: Loss 54.82087326049805\n",
      "Epoch 700: Loss 54.835289001464844\n",
      "Epoch 800: Loss 54.78411865234375\n",
      "Epoch 900: Loss 54.83081817626953\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.001, epochs=2000, batch_size=32, tol=1e-3):\n",
    "        self.learning_rate = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.tolerance = tol\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        return tf.matmul(X, self.weights) + self.bias\n",
    "\n",
    "    def mean_squared_error(self, y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "    def gradient(self, X_batch, y_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.predict(X_batch)\n",
    "            loss = self.mean_squared_error(y_batch, y_pred)\n",
    "        gradient_weights, gradient_bias = tape.gradient(loss, [self.weights, self.bias])\n",
    "        return gradient_weights, gradient_bias\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = tf.Variable(tf.random.normal((n_features, 1)))\n",
    "        self.bias = tf.Variable(tf.random.normal(()))\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            indices = tf.random.shuffle(tf.range(n_samples))\n",
    "            X_shuffled = tf.gather(X, indices)\n",
    "            y_shuffled = tf.gather(y, indices)\n",
    "\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i+self.batch_size]\n",
    "                y_batch = y_shuffled[i:i+self.batch_size]\n",
    "\n",
    "                gradient_weights, gradient_bias = self.gradient(X_batch, y_batch)\n",
    "                # Gradient clipping\n",
    "                gradient_weights = tf.clip_by_value(gradient_weights, -1, 1)\n",
    "                gradient_bias = tf.clip_by_value(gradient_bias, -1, 1)\n",
    "                \n",
    "                self.weights.assign_sub(self.learning_rate * gradient_weights)\n",
    "                self.bias.assign_sub(self.learning_rate * gradient_bias)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                y_pred = self.predict(X)\n",
    "                loss = self.mean_squared_error(y, y_pred)\n",
    "                print(f\"Epoch {epoch}: Loss {loss}\")\n",
    "\n",
    "            if tf.norm(gradient_weights) < self.tolerance:\n",
    "                print(\"Convergence reached.\")\n",
    "                break\n",
    "\n",
    "        return self.weights.numpy(), self.bias.numpy()\n",
    "\n",
    "# Create random dataset with 100 rows and 5 columns\n",
    "X = np.random.randn(100, 5).astype(np.float32)\n",
    "# Create corresponding target value by adding random\n",
    "# noise in the dataset\n",
    "y = np.dot(X, np.array([1, 2, 3, 4, 5], dtype=np.float32)) + np.random.randn(100).astype(np.float32) * 0.1\n",
    "\n",
    "# Create an instance of the SGD class\n",
    "model = SGD(lr=0.005, epochs=1000, batch_size=12, tol=1e-3)\n",
    "w, b = model.fit(X, y)\n",
    "\n",
    "# Predict using predict method from model\n",
    "y_pred = np.dot(X, w) + b\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
