{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize the Model: \n",
    "Start with a simple model, typically just the mean of the target variable.\n",
    "\n",
    "2. Calculate Residuals:\n",
    "For each training example, calculate the difference between the actual target value and the predicted value (i.e., the residuals).\n",
    "\n",
    "3. Train a Weak Learner:\n",
    "A new weak learner (typically a shallow decision tree) is trained to predict the residuals from the previous step.\n",
    "\n",
    "4. Update the Model:\n",
    "The new model is added to the ensemble, adjusting the predictions to better fit the residuals (errors) from the previous round using gradient descent.\n",
    "\n",
    "5. Repeat:\n",
    "The process is repeated, with each new model trying to reduce the remaining error.\n",
    "\n",
    "6. Final Model:\n",
    "The final prediction is a sum of all the predictions from the weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error : L(f)= ∑ L(yi,f(xi)), typically mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the function   \n",
    "f(x) that minimizes the loss function across the dataset:\n",
    "\n",
    "𝑓^0 (𝑥) = argminf  ∑ 𝐿(𝑦𝑖 , 𝑓(𝑥𝑖) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial model is typically a simple prediction, like the mean of the target values in regression tasks:\n",
    "\n",
    "𝑓^0 (𝑥) =  (1/N) * ∑𝑦𝑖\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting with trees \n",
    "\n",
    "\n",
    "To improve the prediction, the algorithm iteratively adds new models (often decision trees) that is trained on the errors made by the previous model.\n",
    "\n",
    "The model at stage m is represented as:\n",
    "\n",
    "F𝑚+1(𝑥𝑖)  = F𝑚(𝑥𝑖) + h𝑚(𝑥𝑖)\n",
    "\n",
    "\n",
    "Here:\n",
    "\n",
    "𝐹𝑚(𝑥𝑖) is the prediction from the previous stage.\n",
    "ℎ𝑚(𝑥𝑖) is the new \"correction\" model added at stage m, which is trained to predict the residuals (the errors) from the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest Descent \n",
    "\n",
    "The key idea of gradient boosting is that it applies the gradient descent optimization method to minimize the loss function.\n",
    "\n",
    "* At each stage m, the new model h𝑚 (𝑥𝑖) should try to reduce the residual errors from the previous stage.\n",
    "* The residual error is the gradient of the loss function with respect to the predictions.   \n",
    "The gradient for sample i at stage 𝑚 is:\n",
    "    \n",
    "    g𝑖𝑚 = − [ (∂𝐿(𝑦𝑖,𝑓(𝑥𝑖))) / ∂𝑓(𝑥𝑖) ]subscript(𝑓(𝑥𝑖) = 𝑓𝑚−1(𝑥𝑖)\n",
    "​\n",
    " \n",
    "This means that the gradient \n",
    "g𝑖𝑚 measures how much the loss would decrease if the prediction at 𝑥𝑖 were adjusted.\n",
    "\n",
    "##### New Model h𝑚(𝑥)\n",
    "We want the new model  ℎ𝑚(𝑥) to be proportional to the negative gradient (steepest descent):\n",
    "\n",
    "ℎ𝑚(𝑥) = − 𝜌𝑚𝑔𝑚\n",
    " \n",
    "where  𝜌𝑚ρm is the step size (or learning rate) that determines how much of the correction (negative gradient) should be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimize Loss with Each New Model\n",
    "\n",
    "At each iteration, we aim to find the new model \n",
    "ℎ𝑚 (𝑥) that reduces the overall loss the most:\n",
    "\n",
    "ℎ𝑚 = argmin ∑ 𝐿(𝑦𝑖, 𝐹𝑚−1(𝑥𝑖) + ℎ𝑚(𝑥𝑖))\n",
    "\n",
    "This means we are finding the function \n",
    "ℎ𝑚(𝑥) that best fits the residuals (the negative gradients) from the previous model. In practice, ℎ𝑚(𝑥 is often a shallow decision tree trained on these residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Final Model\n",
    "After 𝑀 stages, the final model is the sum of all the weak learners (trees) added at each stage:\n",
    "\n",
    "𝑓𝑀 (𝑥) = 𝑓0 (𝑥) + ∑[𝑚=1 to 𝑀] 𝜌𝑚ℎ𝑚 (𝑥)\n",
    "\n",
    "\n",
    "In each iteration, we are adding a new tree that tries to correct the mistakes made by the previous trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean Square error: 56.39\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_diabetes\n",
    " \n",
    "# Setting SEED for reproducibility\n",
    "SEED = 23\n",
    " \n",
    "# Importing the dataset \n",
    "X, y = load_diabetes(return_X_y=True)\n",
    " \n",
    "# Splitting dataset\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, \n",
    "                                                    test_size = 0.25, \n",
    "                                                    random_state = SEED)\n",
    " \n",
    "# Instantiate Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(loss='absolute_error',\n",
    "                                learning_rate=0.1,\n",
    "                                n_estimators=300,\n",
    "                                max_depth = 1, \n",
    "                                random_state = SEED,\n",
    "                                max_features = 5)\n",
    " \n",
    "# Fit to training set\n",
    "gbr.fit(train_X, train_y)\n",
    " \n",
    "# Predict on test set\n",
    "pred_y = gbr.predict(test_X)\n",
    " \n",
    "# test set RMSE\n",
    "test_rmse = mean_squared_error(test_y, pred_y) ** (1 / 2)\n",
    " \n",
    "# Print rmse\n",
    "print('Root mean Square error: {:.2f}'.format(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sequential nature allows GBT to learn complex relationships in the data but makes it more prone to overfitting, especially if not properly regularized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
