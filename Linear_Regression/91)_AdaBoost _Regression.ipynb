{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Idea:\n",
    "AdaBoost focuses on improving the accuracy of a weak learner (such as a shallow decision tree) by adjusting the importance (weight) of data points that were misclassified in previous iterations.\n",
    "\n",
    "### How AdaBoost Works:\n",
    "1. Initialize Weights:\n",
    "Initially, each data point in the training set is given equal weight.\n",
    "2. Train a Weak Learner:\n",
    "A weak learner (e.g., a shallow decision tree, shallow means depth is limited) is trained on the data.\n",
    "\n",
    "3. Calculate Error:\n",
    "The modelâ€™s error is calculated based on the weighted misclassifications. Points that are misclassified get a higher weight in the next round.\n",
    "\n",
    "4. Update Weights:\n",
    "The misclassified points are given higher weights, meaning they will be \"focused\" on more in the next iteration. Correctly classified points get reduced weights.\n",
    "\n",
    "5. Repeat:\n",
    "Another weak learner is trained, now focusing more on the misclassified points. This process repeats, with each new learner trying to correct the errors made by the previous ones.\n",
    "\n",
    "6. Final Model:\n",
    "The final prediction is a weighted sum of the predictions from all weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost is more susceptible to noise and outliers in the data, as it assigns high weights to misclassified samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
