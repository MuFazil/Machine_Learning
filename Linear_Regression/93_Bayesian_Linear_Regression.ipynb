{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a popular regression approach in machine learning. Linear regression is based on the assumption that the underlying data is normally distributed and that all relevant predictor variables have a linear relationship with the outcome.  But In the real world, this is not always possible, it will follows these assumptions, Bayesian regression could be the better choice.\n",
    "Bayesian regression employs prior belief or knowledge about the data to “learn” more about it and create more accurate predictions. It also takes into account the data’s uncertainty and leverages prior knowledge to provide more precise estimates of the data. As a result, it is an ideal choice when the data is complex or ambiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uncertainty in Predictions: Bayesian regression provides not just point estimates for the predictions but also confidence intervals that account for uncertainty in both the data and the model parameters.\n",
    "\n",
    "* Flexibility with Priors: You can incorporate prior knowledge into your model. For instance, if you have some expert knowledge about the expected range of parameters, you can encode this into the prior.\n",
    "\n",
    "* Posterior Distribution: Instead of giving a single set of coefficients for the linear model, Bayesian regression provides a distribution over possible coefficients, which helps quantify the uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive\n",
    "from pyro.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Generate some sample data\n",
    "torch.manual_seed(0)\n",
    "X = torch.linspace(0, 10, 100)\n",
    "true_slope = 2\n",
    "true_intercept = 1\n",
    "Y = true_intercept + true_slope * X + torch.randn(100)\n",
    "\n",
    "# Define the Bayesian regression model\n",
    "def model(X, Y):\n",
    "\t# Priors for the parameters\n",
    "\tslope = pyro.sample(\"slope\", dist.Normal(0, 10))\n",
    "\tintercept = pyro.sample(\"intercept\", dist.Normal(0, 10))\n",
    "\tsigma = pyro.sample(\"sigma\", dist.HalfNormal(1))\n",
    "\n",
    "\t# Expected value of the outcome\n",
    "\tmu = intercept + slope * X\n",
    "\n",
    "\t# Likelihood (sampling distribution) of the observations\n",
    "\twith pyro.plate(\"data\", len(X)):\n",
    "\t\tpyro.sample(\"obs\", dist.Normal(mu, sigma), obs=Y)\n",
    "\n",
    "# Run Bayesian inference using SVI (Stochastic Variational Inference)\n",
    "def guide(X, Y):\n",
    "\t# Approximate posterior distributions for the parameters\n",
    "\tslope_loc = pyro.param(\"slope_loc\", torch.tensor(0.0))\n",
    "\tslope_scale = pyro.param(\"slope_scale\", torch.tensor(1.0),\n",
    "\t\t\t\t\t\t\tconstraint=dist.constraints.positive)\n",
    "\tintercept_loc = pyro.param(\"intercept_loc\", torch.tensor(0.0))\n",
    "\tintercept_scale = pyro.param(\"intercept_scale\", torch.tensor(1.0),\n",
    "\t\t\t\t\t\t\t\tconstraint=dist.constraints.positive)\n",
    "\tsigma_loc = pyro.param(\"sigma_loc\", torch.tensor(1.0), \n",
    "\t\t\t\t\t\tconstraint=dist.constraints.positive)\n",
    "\n",
    "\t# Sample from the approximate posterior distributions\n",
    "\tslope = pyro.sample(\"slope\", dist.Normal(slope_loc, slope_scale))\n",
    "\tintercept = pyro.sample(\"intercept\", dist.Normal(intercept_loc, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tintercept_scale))\n",
    "\tsigma = pyro.sample(\"sigma\", dist.HalfNormal(sigma_loc))\n",
    "\n",
    "# Initialize the SVI and optimizer\n",
    "optim = Adam({\"lr\": 0.01})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())\n",
    "\n",
    "# Run the inference loop\n",
    "num_iterations = 1000\n",
    "for i in range(num_iterations):\n",
    "\tloss = svi.step(X, Y)\n",
    "\tif (i + 1) % 100 == 0:\n",
    "\t\tprint(f\"Iteration {i + 1}/{num_iterations} - Loss: {loss}\")\n",
    "\n",
    "# Obtain posterior samples using Predictive\n",
    "predictive = Predictive(model, guide=guide, num_samples=1000)\n",
    "posterior = predictive(X, Y)\n",
    "\n",
    "# Extract the parameter samples\n",
    "slope_samples = posterior[\"slope\"]\n",
    "intercept_samples = posterior[\"intercept\"]\n",
    "sigma_samples = posterior[\"sigma\"]\n",
    "\n",
    "# Compute the posterior means\n",
    "slope_mean = slope_samples.mean()\n",
    "intercept_mean = intercept_samples.mean()\n",
    "sigma_mean = sigma_samples.mean()\n",
    "\n",
    "# Print the estimated parameters\n",
    "print(\"Estimated Slope:\", slope_mean.item())\n",
    "print(\"Estimated Intercept:\", intercept_mean.item())\n",
    "print(\"Estimated Sigma:\", sigma_mean.item())\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot the posterior distribution of the slope\n",
    "sns.kdeplot(slope_samples, shade=True, ax=axs[0])\n",
    "axs[0].set_title(\"Posterior Distribution of Slope\")\n",
    "axs[0].set_xlabel(\"Slope\")\n",
    "axs[0].set_ylabel(\"Density\")\n",
    "\n",
    "# Plot the posterior distribution of the intercept\n",
    "sns.kdeplot(intercept_samples, shade=True, ax=axs[1])\n",
    "axs[1].set_title(\"Posterior Distribution of Intercept\")\n",
    "axs[1].set_xlabel(\"Intercept\")\n",
    "axs[1].set_ylabel(\"Density\")\n",
    "\n",
    "# Plot the posyterior distribution of sigma\n",
    "sns.kdeplot(sigma_samples, shade=True, ax=axs[2])\n",
    "axs[2].set_title(\"Posterior Distribution of Sigma\")\n",
    "axs[2].set_xlabel(\"Sigma\")\n",
    "axs[2].set_ylabel(\"Density\")\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Very effective when the size of the dataset is small.\n",
    "* Particularly well-suited for on-line based learning (data is received in real-time), as compared to batch-based learning, where we have the entire dataset on our hands before we start training the model. This is because Bayesian Regression doesn’t need to store data.\n",
    "The Bayesian approach is a tried and tested approach and is very robust, mathematically. So, one can use this without having any extra prior knowledge about the dataset.\n",
    "* Bayesian regression methods employ skewed distributions that let you include outside information in your model. \n",
    "\n",
    "\n",
    "#### But,\n",
    "* The inference of the model can be time-consuming.\n",
    "* If there is a large amount of data available for our dataset, the Bayesian approach is not worth it and the regular frequentist approach does a more efficient job\n",
    "* If your code is in a setting where installing new packages is challenging and there are strong dependency controls, this might be an issue.\n",
    "* Many of the basic mistakes that commonly plague traditional frequentist regression models still apply to Bayesian models. For instance, Bayesian models still depend on the linearity of the relationship between the characteristics and the outcome variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
