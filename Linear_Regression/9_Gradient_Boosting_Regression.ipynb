{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize the Model: \n",
    "Start with a simple model, typically just the mean of the target variable.\n",
    "\n",
    "2. Calculate Residuals:\n",
    "For each training example, calculate the difference between the actual target value and the predicted value (i.e., the residuals).\n",
    "\n",
    "3. Train a Weak Learner:\n",
    "A new weak learner (typically a shallow decision tree) is trained to predict the residuals from the previous step.\n",
    "\n",
    "4. Update the Model:\n",
    "The new model is added to the ensemble, adjusting the predictions to better fit the residuals (errors) from the previous round using gradient descent.\n",
    "\n",
    "5. Repeat:\n",
    "The process is repeated, with each new model trying to reduce the remaining error.\n",
    "\n",
    "6. Final Model:\n",
    "The final prediction is a sum of all the predictions from the weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error : L(f)= âˆ‘ L(yi,f(xi)), typically mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the function   \n",
    "f(x) that minimizes the loss function across the dataset:\n",
    "\n",
    "ğ‘“^0 (ğ‘¥) = argminf  âˆ‘ ğ¿(ğ‘¦ğ‘– , ğ‘“(ğ‘¥ğ‘–) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial model is typically a simple prediction, like the mean of the target values in regression tasks:\n",
    "\n",
    "ğ‘“^0 (ğ‘¥) =  (1/N) * âˆ‘ğ‘¦ğ‘–\n",
    "â€‹\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting with trees \n",
    "\n",
    "\n",
    "To improve the prediction, the algorithm iteratively adds new models (often decision trees) that is trained on the errors made by the previous model.\n",
    "\n",
    "The model at stage m is represented as:\n",
    "\n",
    "Fğ‘š+1(ğ‘¥ğ‘–)  = Fğ‘š(ğ‘¥ğ‘–) + hğ‘š(ğ‘¥ğ‘–)\n",
    "\n",
    "\n",
    "Here:\n",
    "\n",
    "ğ¹ğ‘š(ğ‘¥ğ‘–) is the prediction from the previous stage.\n",
    "â„ğ‘š(ğ‘¥ğ‘–) is the new \"correction\" model added at stage m, which is trained to predict the residuals (the errors) from the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest Descent \n",
    "\n",
    "The key idea of gradient boosting is that it applies the gradient descent optimization method to minimize the loss function.\n",
    "\n",
    "* At each stage m, the new model hğ‘š (ğ‘¥ğ‘–) should try to reduce the residual errors from the previous stage.\n",
    "* The residual error is the gradient of the loss function with respect to the predictions.   \n",
    "The gradient for sample i at stage ğ‘š is:\n",
    "    \n",
    "    gğ‘–ğ‘š = âˆ’ [ (âˆ‚ğ¿(ğ‘¦ğ‘–,ğ‘“(ğ‘¥ğ‘–))) / âˆ‚ğ‘“(ğ‘¥ğ‘–) ]subscript(ğ‘“(ğ‘¥ğ‘–) = ğ‘“ğ‘šâˆ’1(ğ‘¥ğ‘–)\n",
    "â€‹\n",
    " \n",
    "This means that the gradient \n",
    "gğ‘–ğ‘š measures how much the loss would decrease if the prediction at ğ‘¥ğ‘– were adjusted.\n",
    "\n",
    "##### New Model hğ‘š(ğ‘¥)\n",
    "We want the new model  â„ğ‘š(ğ‘¥) to be proportional to the negative gradient (steepest descent):\n",
    "\n",
    "â„ğ‘š(ğ‘¥) = âˆ’ ğœŒğ‘šğ‘”ğ‘š\n",
    " \n",
    "where  ğœŒğ‘šÏm is the step size (or learning rate) that determines how much of the correction (negative gradient) should be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimize Loss with Each New Model\n",
    "\n",
    "At each iteration, we aim to find the new model \n",
    "â„ğ‘š (ğ‘¥) that reduces the overall loss the most:\n",
    "\n",
    "â„ğ‘š = argmin âˆ‘ ğ¿(ğ‘¦ğ‘–, ğ¹ğ‘šâˆ’1(ğ‘¥ğ‘–) + â„ğ‘š(ğ‘¥ğ‘–))\n",
    "\n",
    "This means we are finding the function \n",
    "â„ğ‘š(ğ‘¥) that best fits the residuals (the negative gradients) from the previous model. In practice, â„ğ‘š(ğ‘¥ is often a shallow decision tree trained on these residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Final Model\n",
    "After ğ‘€ stages, the final model is the sum of all the weak learners (trees) added at each stage:\n",
    "\n",
    "ğ‘“ğ‘€ (ğ‘¥) = ğ‘“0 (ğ‘¥) + âˆ‘[ğ‘š=1 to ğ‘€] ğœŒğ‘šâ„ğ‘š (ğ‘¥)\n",
    "\n",
    "\n",
    "In each iteration, we are adding a new tree that tries to correct the mistakes made by the previous trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean Square error: 56.39\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_diabetes\n",
    " \n",
    "# Setting SEED for reproducibility\n",
    "SEED = 23\n",
    " \n",
    "# Importing the dataset \n",
    "X, y = load_diabetes(return_X_y=True)\n",
    " \n",
    "# Splitting dataset\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, \n",
    "                                                    test_size = 0.25, \n",
    "                                                    random_state = SEED)\n",
    " \n",
    "# Instantiate Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(loss='absolute_error',\n",
    "                                learning_rate=0.1,\n",
    "                                n_estimators=300,\n",
    "                                max_depth = 1, \n",
    "                                random_state = SEED,\n",
    "                                max_features = 5)\n",
    " \n",
    "# Fit to training set\n",
    "gbr.fit(train_X, train_y)\n",
    " \n",
    "# Predict on test set\n",
    "pred_y = gbr.predict(test_X)\n",
    " \n",
    "# test set RMSE\n",
    "test_rmse = mean_squared_error(test_y, pred_y) ** (1 / 2)\n",
    " \n",
    "# Print rmse\n",
    "print('Root mean Square error: {:.2f}'.format(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sequential nature allows GBT to learn complex relationships in the data but makes it more prone to overfitting, especially if not properly regularized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
